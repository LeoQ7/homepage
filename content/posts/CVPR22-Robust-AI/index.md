---
weight: 5
title: "CVPR 2022 Robust Models towards Openworld Classification Competition Writeup"
date: 2022-06-02T17:55:28+08:00
lastmod: 2022-06-02T17:55:28+08:00
draft: false
author: "Qi Qin"
authorLink: "https://leoq7.com"
description: "Guide to emoji usage in Hugo and LoveIt."
images: []
# resources:
# - name: "featured-image"
#   src: "featured-image.jpg"

tags: ["AI", "CVPR"]
categories: ["AI"]

twemoji: false
lightgallery: true
---

Art of Robustness Workshop on CVPR 2022 and SenseTime jointly organized a Robust AI Competition. This competition focuses on classification task defense and open set defense against adversarial attacks. Our team hit the top 5 in the final round of the competition.

<!--more-->

## Competition Analysis

During the competition, we mainly focused on Track II: Open Set Defense.

> Most defense methods aim to build robust model in the closed set (e.g., under fixed datasets with constrained perturbation types and budgets). However, in the real-world scenario, adversaries would bring more harms and challenges to the deep learning-based applications by generating unrestricted attacks, such as large and visible noises, perturbed images with unseen labels, etc.

> To accelerate the research on building robust models in the open set, we organize this challenge track. Participants are encouraged to develop a robust detector that could distinguish clean examples from perturbed ones on unseen noises and classes by training on a limited-scale dataset.

In essence, it's an adversarial example detection competition that requires participants to determine whether a input image is a benign sample or an adversarial example. In this area, our research center ([SSC](https://ssc.sist.shanghaitech.edu.cn/)) has published two related papers [\[1\]](#Reference)[\[2\]](#Reference), both of which use the similar idea that different kinds of adversarial samples will have unique characteristics.

In [\[1\]](#Reference), the authors use an auto-encoder to pre-process or detect the input. If the adversarial perturbation on the origin image is large, it could be detected directly based on the reconstruction error. If the input has a small perturbation, the noise can be eliminated and changed to a benign sample after decoding. In [\[2\]](#Reference), robustness is used to filter out those samples that just cross the decision boundary, followed by other methods to filter out those large perturbation samples.

In this competition, we use the same strategy where we first classify the potential adversarial examples and use different detection methods for different types of adversarial examples. We divided the adversarial examples into four categories, namely Patch, Square, UAP or FGSM, and Others. In a locally constructed dataset of about 2000 adversarial examples and 18000 benign samples, there are about 250 images of Patch, 250 images of Square, 500 UAP or FGSM images and the other 1000 images generated by adversarial attacks that seek to minimize the L2 perturbations.
 
## Approach

As mentioned earlier, we used different detection methods for different types of adversarial examples, which we will specify here.

- For patch samples, we trained a self-encoder and used the reconstruction error of Linf distance as a judgment metric. As the patch adversarial examples have a significant noise in a region about 20*20, the reconstruction error tends to be close to 1 for that type of adversarial examples, while the reconstruction error for benign samples is closer to 0.

- For square samples, a distinctive feature is the presence of some colored vertical stripes and rectangles visible to the naked eye on the picture. And we noted that the H channel has significant vertical stripes after converting the color space to HSV space. Therefore, we further calculated the difference of the sum of each column. The larger the difference indicates that the corresponding image is more likely to be a square sample.

- For UAP and FGSM samples, they are characterized by the presence of some noise or continuous texture on the image. To make these features more prominent, we used a Laplacian operator to enhance the edges and subsequently enhance the effect of the noise. Similar to square samples, we observed that the noise in the V channel is more pronounced after transforming the images onto HSV space. To measure the amount of noise, we computed histograms for the V channel. The more the curve on the histograms plot is skewed in the positive direction of the x-axis, the more noisy the image is, and the more likely it is a UAP or FGSM sample. Therefore, we constructed an expression to determine the shape of the histogram curve and subsequently whether the image is an adversarial sample.

- For other samples, we directly utilize the metrics used for square detection as the organizer will find out the best threshold that yields the highest F1-Score, rather than requiring us to make a binary classification. Our intention was to use the model output or sample robustness to make judgments, just as [2], but unfortunately the organizers of this competition did not provide model information, so these adversarial examples may have poor transferability against our locally trained model.

## Ablation Study

We constructed the dataset locally and wrote the algorithm to test it against the above modules. 

- For patch attack, the local F1 score on all adversarial examples is 0.2792, where the precision is 0.9829 and the recall is 0.1627. The F1 score for patch is 0.9812, where the recall of patch adv is 0.9795.

- For square attack, the local F1 score on all adversarial examples is 0.2729, where the precision is 0.932 and the recall is 0.1609. The F1 score for patch is 0.8007, where the recall of patch adv is 0.8614.

- For UAP or FGSM attack, the local F1 score on all adversarial examples is 0.478, where the precision is 0.8483 and the recall is 0.3328. The F1 score for patch is 0.8711, where the recall of patch adv is 0.8951.
 
## Reference

[1] Meng, Dongyu, and Hao Chen. "Magnet: a two-pronged defense against adversarial examples." CCS 2017.
[2] Zhao, Zhe, et al. "Attack as defense: characterizing adversarial examples using robustness." ISSTA 2021.